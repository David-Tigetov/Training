\chapter{Оценка ковариационной матрицы}

\section{Оценивание}

Ковариационная матрица $\variance{X}$ неизвестна, но её можно оценить: нужно взять $m$ моментов времени и в каждый из моментов определить состояние приёмников $X_k$
($k = \overline{1,m}$):
\[
    X_k =
    \begin{pmatrix}
        x_{1,1} \\
        \dots   \\
        x_{i,k} \\
        \dots   \\
        x_{j,k} \\
        \dots   \\
        x_{n,k}
    \end{pmatrix} .
\]
Ковариация двух компонент $x_i$ и $x_j$:
\[
    \covariance{x_i}{x_j}
    = \expectation{\left ( x_i - \expectation{x_i} \right ) \left ( x_j - \expectation{x_j} \right )^H}
    = \expectation{ x_i x_j^H},
\]
поскольку $\expectation{x_k} = 0$.

В качестве оценки используем выражение:
\begin{multline*}
    \widehat{\covariance{x_i}{x_j}}
    = \frac{1}{m} \sum_{k=1}^m x_{i,k} x_{j,k}^H
    = \sum_{k=1}^m \frac{1}{\sqrt{m}}x_{i,k} \frac{1}{\sqrt{m}} x_{j,k}^H = \\
    %
    = \frac{1}{\sqrt{m}}
    \begin{pmatrix}
        x_{i,1} & x_{i,2} & \dots & x_{i,m}
    \end{pmatrix}
    \frac{1}{\sqrt{m}}
    \begin{pmatrix}
        x_{j,1}^H \\
        x_{j,2}^H \\
        \dots     \\
        x_{j,m}^H
    \end{pmatrix}
\end{multline*}
Все оценки ковариаций можно получить умножением матриц:
Полученные векторы объединяем в матрицу $Y$:
\[
    \widehat{R} =
    \frac{1}{\sqrt{m}}
    \begin{pmatrix}
        x_{1,1} & x_{1,2} & \dots  & x_{1,m} \\
        x_{2,1} & x_{2,2} & \dots  & x_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        x_{n,1} & x_{n,2} & \dots  & x_{n,m}
    \end{pmatrix}
    \frac{1}{\sqrt{m}}
    \begin{pmatrix}
        x_{1,1}^* & x_{2,2}^* & \dots  & x_{n,m}^* \\
        x_{1,2}^* & x_{2,2}^* & \dots  & x_{n,m}^* \\
        \vdots    & \vdots    & \ddots & \vdots    \\
        x_{1,m}^* & x_{2,m}^* & \dots  & x_{n,m}^*
    \end{pmatrix}
    .
\]
Правая матрица является эрмитовосопряженной к левой матрице, поэтому если:
\[
    Y =
    \frac{1}{\sqrt{m}}
    \begin{pmatrix}
        x_{1,1} & x_{1,2} & \dots  & x_{1,m} \\
        x_{2,1} & x_{2,2} & \dots  & x_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        x_{n,1} & x_{n,2} & \dots  & x_{n,m}
    \end{pmatrix} ,
\]
тогда
\[
    \widehat{R} = Y Y^H .
\]


\section{Ортогонализация и обращение}

Матрица $\widehat{R}$ является факторизованной, поэтому можно найти факторизацию обратной матрицы $\widehat{R}^{-1}$.

Пусть $\Phi$ является преобразованием, ортогонализующим строки матрицы $Y$, то есть строки матрицы $\Phi Y$ являются взаимно ортогональными:
\[
    \left ( \Phi Y \right ) \left ( \Phi Y \right )^H = I_n ,
\]
отсюда
\begin{gather*}
    \Phi Y Y^H \Phi^H = I_n , \\
    \Phi \widehat{R} \Phi^H = I_n , \\
    \Phi \widehat{R} = \Phi^{-H}, \\
    \widehat{R} = \Phi^{-1} \Phi^{-H}, \\
    \widehat{R}^{-1} = \left ( \Phi^{-1} \Phi^{-H} \right )^{-1}, \\
    \widehat{R}^{-1} = \Phi^H \Phi .
\end{gather*}

\section{Вычисления}

Вычисление квадратичной формы:
\[
    V^H \widehat{R}^{-1} V
    = V^H \Phi^H \Phi V
    = \left ( \Phi V \right )^H \Phi V
    = \norm{\Phi V}^2 .
\]
Вычисление оптимального весового вектора:
\[
    W_{max}
    = \widehat{R}^{-1} U
    = \Phi^H \Phi U .
\]